# -*- coding: utf-8 -*-
# notify_llm.py
import requests
import json
import datetime
import base64
import sys
import re
import pymysql

WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/c74b9141-1759-40e2-ae3a-50cc6389e1bc"
FEISHU_OPEN_ID = "ou_20b2bd16a8405b93019b7291ec5202c3"

# å¤§æ¨¡å‹è°ƒç”¨é…ç½®ï¼ˆå·²é‰´æƒï¼‰
API_URL = "https://llm-cmt-api.dev.fc.chj.cloud/agentops/chat/completions"
HEADERS = {
    "Content-Type": "application/json",
    # "Authorization": "Bearer <your-token>",
}

# TiDB è¿æ¥ä¿¡æ¯ï¼ˆä¸ç›‘æ§è¡¨åŒåº“ï¼‰
DB_CONFIG = {
    "host": "da-dw-tidb-10900.chj.cloud",
    "port": 3306,
    "user": "da_algo_craw_wr",
    "password": "99FBD18120C777560A9451FB65A8E74F60CFBBD3",
    "database": "da_crawler_dw",
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor
}

# é€šçŸ¥è½åº“çš„ç›®æ ‡è¡¨ï¼ˆæ–°å¢ï¼šsummaryã€event_levelï¼‰
NOTIFY_TABLE = "dwd_idc_life_ent_soc_public_sentiment_battery_work_notify_mix_rt"

# å­—æ®µæ˜ å°„
FIELD_MAP = {
    "summary":      "æ–‡ç« æ‘˜è¦",
    "work_id":      "ä¸»è´´ID",
    "work_url":     "ä¸»è´´é“¾æ¥",
    "work_title":   "ä¸»è´´æ ‡é¢˜",
    "work_content": "æ­£æ–‡å†…å®¹",
    "publish_time": "å‘å¸ƒæ—¶é—´",
    "crawled_time": "æŠ“å–æ—¶é—´",
    "account_name": "ä½œè€…åç§°",
    "source":       "æ¥æºå¹³å°",
    "like_cnt":     "ç‚¹èµæ•°",
    "reply_cnt":    "è¯„è®ºæ•°",
    "forward_cnt":  "è½¬å‘æ•°",
    "content_senti":"å†…å®¹æƒ…æ„Ÿ",
    "ocr_content":  "OCRè¯†åˆ«å†…å®¹"
}

# å±•ç¤ºé¡ºåºï¼ˆåœ¨ä¸»è´´æ ‡é¢˜å‰æ–°å¢â€œæ–‡ç« æ‘˜è¦â€ï¼‰
ORDERED_FIELDS = [
    "source", "work_url", "publish_time", "account_name",
    "summary",
    "work_title", "work_content",
    "like_cnt", "reply_cnt", "forward_cnt"
]

# æ ¹æ®çƒˆåº¦è¾“å‡ºå¤„ç†æ„è§
ADVICE_BY_SEVERITY = {
    "ä½": "è¯·ç›¸å…³äººå‘˜äº†è§£",
    "ä¸­": "è¯·ç›¸å…³äººå‘˜å…³æ³¨",
    "é«˜": "è¯·ç›¸å…³äººå‘˜é‡ç‚¹å…³æ³¨",
}

# ================= å…¬å…±å·¥å…· =================
def double_base64_decode(s: str) -> str:
    try:
        return base64.b64decode(base64.b64decode(s)).decode("utf-8")
    except Exception:
        return f"[è§£ç å¤±è´¥]{s}"

def truncate_text(text: str, limit=200) -> str:
    if text is None:
        return ""
    text = str(text)
    return text[:limit] + "..." if len(text) > limit else text

def map_senti(val):
    try:
        v = int(val)
    except Exception:
        return str(val)
    return {-1: "è´Ÿé¢", 0: "ä¸­æ€§", 1: "æ­£é¢"}.get(v, str(v))

def call_chat_completion_stream(prompt: str, model: str = "azure-gpt-4o") -> str:
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": True
    }

    result_chunks = []
    with requests.post(API_URL, headers=HEADERS, data=json.dumps(payload), stream=True, timeout=300) as resp:
        if resp.status_code != 200:
            raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")

        for raw_line in resp.iter_lines(chunk_size=1024, decode_unicode=False):
            if not raw_line:
                continue
            line = raw_line.strip()
            if not line.startswith(b"data:"):
                continue

            data_bytes = line[len(b"data:"):].strip()
            if data_bytes == b"[DONE]":
                break

            try:
                obj = json.loads(data_bytes.decode("utf-8"))
            except Exception:
                result_chunks.append(data_bytes.decode("utf-8", errors="ignore"))
                continue

            choices = obj.get("choices") or []
            for ch in choices:
                delta = ch.get("delta") or {}
                chunk = delta.get("content")
                if chunk is None:
                    chunk = ch.get("content")
                if chunk is None:
                    msg = ch.get("message") or {}
                    chunk = msg.get("content")
                if chunk:
                    result_chunks.append(chunk)

    return "".join(result_chunks).strip()

def _extract_json_from_text(text: str) -> str:
    s = text.strip()
    fence = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", s, flags=re.IGNORECASE)
    if fence:
        return fence.group(1).strip()
    start = s.find("{")
    if start != -1:
        depth = 0
        for i in range(start, len(s)):
            c = s[i]
            if c == "{":
                depth += 1
            elif c == "}":
                depth -= 1
                if depth == 0:
                    candidate = s[start:i+1]
                    return candidate.strip()
    return s

# ================= å“ç‰Œä¸ä¸»é¢˜è¯æ£€æµ‹ =================
def contains_li_brand(text: str) -> bool:
    """
    ä¸¥æ ¼åŒ¹é…ç†æƒ³æ±½è½¦å“ç‰Œæˆ–è½¦å‹ï¼š
    ç†æƒ³æ±½è½¦/Li Autoï¼›ç†æƒ³ONEï¼›ç†æƒ³L6/L7/L8/L9ï¼›ç†æƒ³i6/i8ï¼›ç†æƒ³MEGA/Mega
    """
    if not text:
        return False
    s = str(text)
    patterns = [
        r"ç†æƒ³æ±½è½¦",
        r"\bli\s*auto\b",            # Li Auto
        r"ç†æƒ³\s*one",               # ç†æƒ³ONE
        r"ç†æƒ³\s*l6", r"ç†æƒ³\s*l7", r"ç†æƒ³\s*l8", r"ç†æƒ³\s*l9",  # ç†æƒ³L6-L9
        r"ç†æƒ³l6", r"ç†æƒ³l7", r"ç†æƒ³l8", r"ç†æƒ³l9",
        r"ç†æƒ³\s*i6", r"ç†æƒ³\s*i8",   # ç†æƒ³i6/i8
        r"ç†æƒ³i6", r"ç†æƒ³i8",
        r"ç†æƒ³\s*mega", r"ç†æƒ³\s*MEGA",  # ç†æƒ³Mega/MEGA
        r"ç†æƒ³mega", r"ç†æƒ³MEGA",
    ]
    return any(re.search(pat, s, flags=re.IGNORECASE) for pat in patterns)

def contains_competitor_brand(text: str) -> bool:
    """
    å‹å•†å“ç‰Œåˆ—è¡¨ï¼ˆå¯æ‰©å……ï¼‰ã€‚åªè¦å‡ºç°ä»»ä¸€å³è§†ä¸ºæœ‰å‹å•†å“ç‰Œä¿¡å·ã€‚
    """
    if not text:
        return False
    s = str(text).lower()
    competitors = [
        # æ–°åŠ¿åŠ›/å›½å†…
        "ç‰¹æ–¯æ‹‰", "tesla",
        "æ¯”äºšè¿ª", "byd",
        "è”šæ¥", "nio",
        "å°é¹", "xpeng",
        "ææ°ª", "zeekr",
        "é—®ç•Œ", "aito", "åä¸º", "huawei", "èµ›åŠ›æ–¯", "seres",
        "æ™ºå·±", "im",
        "å²šå›¾", "voyah",
        "è…¾åŠ¿", "denza",
        "æ·±è“", "changan",
        "å“ªå’", "hozon",
        "é›¶è·‘", "leapmotor",
        "å¹¿æ±½åŸƒå®‰", "åŸƒå®‰", "aion", "gac",
        "å‰åˆ©", "geely",
        "ç†æƒ³ä»¥å¤–å“ç‰Œ",  # å ä½æç¤ºï¼Œæ— å®é™…ä½œç”¨
        # ä¼ ç»Ÿå›½é™…å“ç‰Œ
        "å®é©¬", "bmw",
        "å¥”é©°", "mercedes", "benz",
        "å¥¥è¿ª", "audi",
        "å¤§ä¼—", "vw", "volkswagen",
        "ä¸°ç”°", "toyota",
        "æœ¬ç”°", "honda",
        "æ—¥äº§", "nissan",
        # å…¶å®ƒ
        "æè¶Š", "jiue",
        "æç‹", "arcfox",
        "é•¿åŸ", "great wall",
        "é­ç‰Œ", "wey",
        "å¦å…‹", "tank",
        "å°ç±³æ±½è½¦", "xiaomi", "su7",
    ]
    return any(k in s for k in competitors)

def contains_battery_topic(text: str) -> bool:
    if not text:
        return False
    s = str(text).lower()
    batt_keywords = [
        "ç”µæ± ", "ç»­èˆª", "å……ç”µ", "æ…¢å……", "å¿«å……", "æ¢ç”µ",
        "èµ·ç«", "çˆ†ç‚¸", "æ¼æ¶²", "é¼“åŒ…", "å†…é˜»", "è¡°å‡",
        "ä½æ¸©", "é«˜å‹", "ä½å‹", "bms", "ç”µé‡", "soc", "soh",
        "å®¹é‡", "èƒ½é‡å›æ”¶", "å……ç”µæ¡©", "å……ç”µå£", "å……ç”µæª", "é«˜å‹åŒ…", "ä¸‰ç”µ"
    ]
    return any(k in s for k in batt_keywords)

# ================= ç›¸å…³æ€§åˆ¤å®šï¼ˆæ¨é€é—¨ç¦ï¼‰ =================
def build_related_gate_prompt(title: str, content: str, ocr: str) -> str:
    """
    ä¸ç†æƒ³æ±½è½¦ç”µæ± ç›¸å…³æ€§çš„é—¨ç¦åˆ¤å®šæç¤ºè¯ï¼ˆæ›´ä¸¥æ ¼ç‰ˆï¼‰ã€‚
    ä»…è¿”å›çº¯ JSONï¼š{"related":"æ˜¯"} æˆ– {"related":"å¦"}ã€‚

    ä¸¥æ ¼åˆ¤å®šè§„åˆ™ï¼ˆå¿…é¡»åŒæ—¶æ»¡è¶³ï¼‰ï¼š
    1) æ–‡æœ¬ä¸­æ˜ç¡®å‡ºç°ç†æƒ³æ±½è½¦å“ç‰Œæˆ–è½¦å‹ä¹‹ä¸€ï¼š
       ç†æƒ³æ±½è½¦ / Li Auto / ç†æƒ³ONE / ç†æƒ³L6 / ç†æƒ³L7 / ç†æƒ³L8 / ç†æƒ³L9 / ç†æƒ³i6 / ç†æƒ³i8 / ç†æƒ³MEGA/Megaã€‚
       æ³¨æ„ï¼šå‡ºç°â€œç†æƒ³â€ä¸€è¯ä½†éå“ç‰Œè¯­å¢ƒï¼ˆå¦‚â€œç†æƒ³ç”Ÿæ´»â€â€œç†æƒ³çŠ¶æ€â€ï¼‰ä¸ç®—å“ç‰ŒæŒ‡å‘ã€‚
    2) è¯é¢˜èšç„¦ç”µæ± ç›¸å…³è®®é¢˜ï¼ˆç”µæ± ã€ç»­èˆªã€å……ç”µã€æ…¢å……ã€å¿«å……ã€æ¢ç”µã€å®‰å…¨ã€èµ·ç«ã€çˆ†ç‚¸ã€æ•…éšœã€ä½æ¸©ã€é«˜å‹ã€BMSã€SOCã€SOHã€å®¹é‡ã€èƒ½é‡å›æ”¶ç­‰ï¼‰ã€‚
    3) è‹¥åªå‡ºç°å‹å•†å“ç‰Œï¼ˆå¦‚ç‰¹æ–¯æ‹‰ã€æ¯”äºšè¿ªã€è”šæ¥ã€å°é¹ã€ææ°ªã€é—®ç•Œã€æ™ºå·±ã€å²šå›¾ã€è…¾åŠ¿ã€æ·±è“ã€å“ªå’ã€é›¶è·‘ã€åŸƒå®‰ã€å‰åˆ©ã€å®é©¬ã€å¥”é©°ã€å¥¥è¿ªã€å¤§ä¼—ç­‰ï¼‰
       è€Œæ²¡æœ‰å‡ºç°ç†æƒ³å“ç‰Œæˆ–è½¦å‹ï¼Œåˆ™åˆ¤å®šä¸ºâ€œä¸ç›¸å…³â€ã€‚

    åªè¿”å›çº¯ JSONï¼Œä¸è¦ä»£ç å—æˆ–å…¶ä»–æ–‡å­—ï¼š
    {"related": "æ˜¯"} æˆ– {"related": "å¦"}
    """
    title = title or ""
    content = content or ""
    ocr = ocr or ""
    return (
        "è¯·ä¸¥æ ¼åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸â€œç†æƒ³æ±½è½¦â€çš„ç”µæ± ç›¸å…³ï¼ˆé¿å…æŠŠå‹å•†ç”µæ± è¯é¢˜è¯¯åˆ¤ä¸ºç†æƒ³ï¼‰ã€‚"
        "å¿…é¡»æ»¡è¶³ï¼šå‡ºç°ç†æƒ³æ±½è½¦å“ç‰Œ/è½¦å‹ï¼ˆç†æƒ³æ±½è½¦/Li Auto/ç†æƒ³ONE/ç†æƒ³L6/L7/L8/L9/ç†æƒ³i6/i8/ç†æƒ³MEGAï¼‰ï¼Œä¸”è¯é¢˜é›†ä¸­åœ¨ç”µæ± ç›¸å…³è®®é¢˜ã€‚"
        "è‹¥ä»…å‡ºç°å‹å•†å“ç‰Œï¼ˆå¦‚ç‰¹æ–¯æ‹‰ã€æ¯”äºšè¿ªã€è”šæ¥ã€å°é¹ã€ææ°ªã€é—®ç•Œã€æ™ºå·±ã€å²šå›¾ã€è…¾åŠ¿ã€æ·±è“ã€å“ªå’ã€é›¶è·‘ã€åŸƒå®‰ã€å‰åˆ©ã€å®é©¬ã€å¥”é©°ã€å¥¥è¿ªã€å¤§ä¼—ç­‰ï¼‰è€Œæœªå‡ºç°ç†æƒ³å“ç‰Œæˆ–è½¦å‹ï¼Œåˆ™åˆ¤å®šä¸ºâ€œä¸ç›¸å…³â€ã€‚"
        "åªè¿”å›çº¯ JSONï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡å­—ï¼š"
        '{"related": "æ˜¯"} æˆ– {"related": "å¦"}'
        f"\næ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content}\nOCRï¼š{ocr}\n"
        "åªè¿”å›ä¸Šè¿° JSONã€‚"
    )

def parse_related_json(text: str) -> bool:
    raw = text.strip()
    json_str = _extract_json_from_text(raw)
    related = None
    try:
        obj = json.loads(json_str)
        if isinstance(obj, dict):
            val = str(obj.get("related", "")).strip()
            if val in ("æ˜¯", "å¦"):
                related = (val == "æ˜¯")
    except Exception:
        pass

    if related is None:
        # å›é€€è§„åˆ™ï¼šå“ç‰Œ + ç”µæ± è¯é¢˜åŒæ—¶å‡ºç°
        has_li = contains_li_brand(raw)
        has_batt = contains_battery_topic(raw)
        related = bool(has_li and has_batt)

    return related

def check_related(data: dict) -> bool:
    """
    ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œé—¨ç¦åˆ¤å®šï¼šæ˜¯å¦ä¸ç†æƒ³æ±½è½¦ç”µæ± ç›¸å…³ã€‚
    æ›´ä¸¥æ ¼å…œåº•ï¼šè‹¥æ–‡æœ¬å‡ºç°å‹å•†å“ç‰Œä¸”æœªå‡ºç°ç†æƒ³å“ç‰Œ/è½¦å‹ï¼Œåˆ™å¼ºåˆ¶åˆ¤å®šä¸ºä¸ç›¸å…³ã€‚
    åŒæ—¶è¦æ±‚å¿…ç„¶å‡ºç°ç”µæ± ç›¸å…³è¯é¢˜ã€‚
    """
    title = data.get("work_title") or ""
    content = data.get("work_content") or ""
    ocr = data.get("ocr_content") or ""
    raw_text = f"{title}\n{content}\n{ocr}"

    # å¿«é€Ÿå…œåº•ï¼šå‡ºç°å‹å•†ä¸”æ²¡æœ‰ç†æƒ³å“ç‰Œ/è½¦å‹ => ä¸ç›¸å…³
    if contains_competitor_brand(raw_text) and not contains_li_brand(raw_text):
        return False

    # é¿å…éå­—ç¬¦ä¸²ç±»å‹å¯¼è‡´æç¤ºè¯å¼‚å¸¸
    t = title if isinstance(title, str) else json.dumps(title, ensure_ascii=False)
    c = content if isinstance(content, str) else json.dumps(content, ensure_ascii=False)
    o = ocr if isinstance(ocr, str) else json.dumps(ocr, ensure_ascii=False)

    prompt = build_related_gate_prompt(t, c, o)
    try:
        llm_text = call_chat_completion_stream(prompt, model="azure-gpt-4o")
        related_by_llm = parse_related_json(llm_text)
    except Exception:
        related_by_llm = None

    # å¿…é¡»æ¡ä»¶ï¼šç†æƒ³å“ç‰Œ/è½¦å‹ + ç”µæ± è¯é¢˜
    has_li_brand = contains_li_brand(raw_text)
    has_batt = contains_battery_topic(raw_text)
    has_competitor = contains_competitor_brand(raw_text)

    # å¦‚æœæ¨¡å‹è¯´â€œæ˜¯â€ï¼Œä½†å‡ºç°å‹å•†ä¸”æ²¡æœ‰ç†æƒ³å“ç‰Œ/è½¦å‹ï¼Œåˆ™çº æ­£ä¸ºå¦
    if related_by_llm is True and has_competitor and not has_li_brand:
        return False

    # æ¨¡å‹ä¸å¯ç”¨æ—¶ï¼Œç”¨è§„åˆ™åˆ¤å®š
    if related_by_llm is None:
        return bool(has_li_brand and has_batt)

    # æ¨¡å‹ç»“æœä¸º True ä¹Ÿè¦æ»¡è¶³ç¡¬æ€§æ¡ä»¶
    return bool(related_by_llm and has_li_brand and has_batt)

# ================= æ‘˜è¦ä¸çƒˆåº¦ =================
def build_summary_prompt(title: str, content: str, ocr: str) -> str:
    title = title or ""
    content = content or ""
    ocr = ocr or ""
    return (
        "ä½ æ˜¯ä¼ä¸šèˆ†æƒ…åˆ†æåŠ©æ‰‹ã€‚è¯·é˜…è¯»ä¸»è´´æ ‡é¢˜ã€æ­£æ–‡ã€OCRè¯†åˆ«å†…å®¹ã€‚"
        "è¯·å®Œæˆï¼š"
        "1) ç”¨ä¸­æ–‡è¾“å‡ºçº¦50å­—çš„ä¸€æ®µäº‹ä»¶æ‘˜è¦ï¼ˆä¸åŒ…å«æ˜¯å¦ä¸ç†æƒ³æ±½è½¦ç”µæ± ç›¸å…³çš„åˆ¤æ–­ï¼‰ï¼›"
        "2) å•ç‹¬ç»™å‡ºäº‹ä»¶çƒˆåº¦ï¼Œä»…å¯ä¸ºï¼šä½/ä¸­/é«˜ï¼›"
        "ä¸¥æ ¼è¿”å›çº¯ JSON æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡å­—ï¼Œä¸è¦ä½¿ç”¨ä»£ç å—æˆ–åå¼•å·ï¼š"
        '{"summary": "<äº‹ä»¶æ‘˜è¦>", "severity": "<ä½|ä¸­|é«˜>"}'
        f"\næ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content}\nOCRï¼š{ocr}\n"
        "åªè¿”å›ä¸Šè¿° JSONã€‚"
    )

def parse_summary_json(text: str):
    raw = text.strip()
    json_str = _extract_json_from_text(raw)
    summary = None
    severity = None
    try:
        obj = json.loads(json_str)
        if isinstance(obj, dict):
            summary = str(obj.get("summary", "")).strip() or None
            severity = str(obj.get("severity", "")).strip() or None
    except Exception:
        pass

    if summary is None:
        summary = re.sub(r"^```(?:json)?\s*|\s*```$", "", raw, flags=re.IGNORECASE).strip()

    if severity not in ("ä½", "ä¸­", "é«˜"):
        m = re.search(r"(ä½|ä¸­|é«˜)", raw)
        severity = m.group(1) if m else "ä¸­"

    return summary, severity

def generate_summary_and_severity(data: dict):
    title = data.get("work_title") or ""
    content = data.get("work_content") or ""
    ocr = data.get("ocr_content") or ""

    t = title if isinstance(title, str) else json.dumps(title, ensure_ascii=False)
    c = content if isinstance(content, str) else json.dumps(content, ensure_ascii=False)
    o = ocr if isinstance(ocr, str) else json.dumps(ocr, ensure_ascii=False)

    prompt = build_summary_prompt(t, c, o)
    try:
        llm_text = call_chat_completion_stream(prompt, model="azure-gpt-4o")
    except Exception as e:
        return f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥] {e}", "ä¸­"

    summary, severity = parse_summary_json(llm_text)
    return summary, severity

# ================= æ¨é€æ•°æ®è½åº“ =================
def _safe_int(v, default=None):
    if v is None:
        return default
    try:
        return int(v)
    except Exception:
        return default

def save_notify_record_to_tidb(data: dict, summary_text: str, severity: str):
    """
    å°†å³å°†/å·²ç»æ¨é€åˆ°é£ä¹¦çš„æ•°æ®è½åº“åˆ°é€šçŸ¥è¡¨ï¼ˆåŒåº“ï¼‰ã€‚
    ä½¿ç”¨ work_id ä½œä¸ºå”¯ä¸€é”®ï¼Œè‹¥å·²å­˜åœ¨åˆ™æ›´æ–°ã€‚
    """
    try:
        conn = pymysql.connect(**DB_CONFIG)
        conn.autocommit(True)
        with conn.cursor() as cursor:
            sql = f"""
            INSERT INTO {NOTIFY_TABLE} (
                work_id, work_url, work_title, work_content,
                publish_time, crawled_time, account_name, source,
                like_cnt, reply_cnt, forward_cnt, content_senti,
                ocr_content, summary, event_level
            ) VALUES (
                %(work_id)s, %(work_url)s, %(work_title)s, %(work_content)s,
                %(publish_time)s, %(crawled_time)s, %(account_name)s, %(source)s,
                %(like_cnt)s, %(reply_cnt)s, %(forward_cnt)s, %(content_senti)s,
                %(ocr_content)s, %(summary)s, %(event_level)s
            )
            ON DUPLICATE KEY UPDATE
                work_url=VALUES(work_url),
                work_title=VALUES(work_title),
                work_content=VALUES(work_content),
                publish_time=VALUES(publish_time),
                crawled_time=VALUES(crawled_time),
                account_name=VALUES(account_name),
                source=VALUES(source),
                like_cnt=VALUES(like_cnt),
                reply_cnt=VALUES(reply_cnt),
                forward_cnt=VALUES(forward_cnt),
                content_senti=VALUES(content_senti),
                ocr_content=VALUES(ocr_content),
                summary=VALUES(summary),
                event_level=VALUES(event_level)
            """
            params = {
                "work_id": data.get("work_id"),
                "work_url": data.get("work_url"),
                "work_title": data.get("work_title"),
                "work_content": data.get("work_content"),
                "publish_time": data.get("publish_time"),
                "crawled_time": data.get("crawled_time"),
                "account_name": data.get("account_name"),
                "source": data.get("source"),
                "like_cnt": _safe_int(data.get("like_cnt"), default=-99),
                "reply_cnt": _safe_int(data.get("reply_cnt"), default=-99),
                "forward_cnt": _safe_int(data.get("forward_cnt"), default=-99),
                "content_senti": _safe_int(data.get("content_senti")),
                "ocr_content": data.get("ocr_content"),
                "summary": summary_text,
                "event_level": severity
            }
            cursor.execute(sql, params)
            print("âœ… é€šçŸ¥æ•°æ®å·²è½åº“åˆ° TiDB é€šçŸ¥è¡¨")
    except Exception as e:
        print(f"âŒ é€šçŸ¥æ•°æ®è½åº“å¤±è´¥: {e}")
    finally:
        try:
            conn.close()
        except Exception:
            pass

# ================= æ¨é€æµç¨‹ =================
def send_to_feishu(data: dict):
    # é—¨ç¦ï¼šå…ˆåˆ¤æ–­æ˜¯å¦ä¸ç†æƒ³æ±½è½¦ç”µæ± ç›¸å…³ï¼Œä¸ç›¸å…³åˆ™è·³è¿‡æ¨é€å’Œè½åº“
    try:
        if not check_related(data):
            print("è·³è¿‡æ¨é€ä¸è½åº“ï¼šæ¨¡å‹åˆ¤å®šä¸ç†æƒ³æ±½è½¦ç”µæ± ä¸ç›¸å…³ï¼ˆè®°å½•å·²æ–°å¢åˆ°æ•°æ®åº“ï¼‰ã€‚")
            return False
    except Exception as e:
        print(f"ç›¸å…³æ€§åˆ¤å®šå¼‚å¸¸ï¼Œé»˜è®¤è·³è¿‡æ¨é€ä¸è½åº“ï¼š{e}")
        return False

    post_content = []

    # ç”Ÿæˆæ‘˜è¦ä¸çƒˆåº¦ï¼ˆæ‘˜è¦ä¸æˆªæ–­ï¼‰
    summary_text, severity = generate_summary_and_severity(data)
    advice = ADVICE_BY_SEVERITY.get(severity, ADVICE_BY_SEVERITY["ä¸­"])

    for k in ORDERED_FIELDS:
        if k == "summary":
            v = summary_text
        else:
            if k not in data:
                continue
            v = data.get(k)

            if isinstance(v, datetime.datetime):
                v = v.strftime("%Y-%m-%d %H:%M:%S")
            if v is None:
                v = ""

            if k in ("work_title", "work_content", "ocr_content"):
                v = truncate_text(v, limit=200)
            if k == "account_name":
                v = double_base64_decode(v)
            if k == "content_senti":
                v = map_senti(v)

        label = FIELD_MAP.get(k, k)
        post_content.append([
            {"tag": "text", "text": f"ã€{label}ã€‘: {v}"}
        ])

    post_content.append([
        {"tag": "at", "user_id": FEISHU_OPEN_ID},
        {"tag": "text", "text": f" {advice}ï¼ˆçƒˆåº¦ï¼š{severity}ï¼‰"}
    ])

    payload = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": "ğŸ“¢ æ–°å¢è®°å½•å‘Šè­¦",
                    "content": post_content
                }
            }
        }
    }

    ok = False
    try:
        resp = requests.post(
            WEBHOOK_URL,
            headers={"Content-Type": "application/json; charset=utf-8"},
            data=json.dumps(payload, ensure_ascii=False).encode("utf-8")
        )
        if resp.status_code == 200:
            try:
                j = resp.json()
                if j.get("StatusCode") == 0 or j.get("code") == 0:
                    ok = True
            except Exception:
                ok = True
        if ok:
            print("âœ… é£ä¹¦æ¶ˆæ¯å·²å‘é€å¹¶@æŒ‡å®šäºº")
        else:
            print(f"âŒ é£ä¹¦æ¶ˆæ¯å‘é€å¤±è´¥: {resp.status_code} {resp.text}")
    except Exception as e:
        print(f"âŒ è°ƒç”¨é£ä¹¦æ¥å£å¼‚å¸¸: {e}")

    # æ¨é€æˆåŠŸåè½åº“ï¼ˆå¦‚éœ€ç›¸å…³å³è½åº“ï¼Œå¯æ”¹ä¸ºæ— è®º ok ä¸å¦éƒ½æ‰§è¡Œ save_notify_record_to_tidbï¼‰
    if ok:
        try:
            save_notify_record_to_tidb(data, summary_text, severity)
        except Exception as e:
            print(f"âŒ æ¨é€åè½åº“å¼‚å¸¸: {e}")

    return ok

# ================= è¿è¡Œå…¥å£ =================
if __name__ == "__main__":
    if len(sys.argv) >= 2:
        try:
            row_json_str = sys.argv[1]
            data = json.loads(row_json_str)
            send_to_feishu(data)
        except Exception as e:
            print(f"âŒ è§£æè¾“å…¥ JSON å¤±è´¥: {e}")
            sys.exit(1)
    else:
        # ç¤ºä¾‹æµ‹è¯•æ•°æ®ï¼ˆä»…ç‹¬ç«‹è¿è¡Œæ—¶ä½¿ç”¨ï¼‰
        test_data = {
            "id": 186,
            "work_id": "315bd20e7e7690e27f2859689ac4ba04",
            "work_url": "www.baidu.com",
            "work_title": "ç†æƒ³L9åœ¨å¯’æ½®ä¸­ç»­èˆªè¡°å‡æ˜æ˜¾ï¼Œç”¨æˆ·åé¦ˆå……ç”µæ…¢",
            "work_content": "æœ‰è½¦ä¸»åæ˜ ç†æƒ³L9åœ¨ä½æ¸©ç¯å¢ƒä¸‹ç”µæ± è¡¨ç°ä¸ä½³ï¼Œç»­èˆªä¸‹é™å¹¶ä¸”å……ç”µé€Ÿåº¦æ…¢ï¼Œéœ€è¦ä¼˜åŒ–BMSç­–ç•¥ã€‚",
            "publish_time": datetime.datetime.now(),
            "crawled_time": datetime.datetime.now(),
            "account_name": base64.b64encode(
                base64.b64encode("æµ‹è¯•è´¦å·".encode("utf-8"))
            ).decode("utf-8"),
            "source": "å¾®åš",
            "like_cnt": 99,
            "reply_cnt": 12,
            "forward_cnt": 5,
            "content_senti": 0,
            "ocr_content": "ç†æƒ³æ±½è½¦L9ä½æ¸©ç»­èˆªä¸‹é™å……ç”µæ…¢çš„èˆ†æƒ…æ›å…‰"
        }
        print("æœªæ£€æµ‹åˆ°è¾“å…¥å‚æ•°ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•æ¨é€å¹¶è½åº“...")
        send_to_feishu(test_data)