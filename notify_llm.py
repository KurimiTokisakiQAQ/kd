# -*- coding: utf-8 -*-
# feishu_notify.py
import requests
import json
import datetime
import base64
import sys
import re
import pymysql

# ============ é£ä¹¦ä¸å¤§æ¨¡å‹é…ç½® ============
WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/c74b9141-1759-40e2-ae3a-50cc6389e1bc"
FEISHU_OPEN_ID = "ou_20b2bd16a8405b93019b7291ec5202c3"

API_URL = "https://llm-cmt-api.dev.fc.chj.cloud/agentops/chat/completions"
HEADERS = {
    "Content-Type": "application/json",
}

# ============ TiDB è¿æ¥ä¿¡æ¯ï¼ˆä¸ç›‘æ§ç¨‹åºä¿æŒä¸€è‡´ï¼‰ ============
TIDB_HOST = "da-dw-tidb-10900.chj.cloud"
TIDB_PORT = 3306
TIDB_USER = "da_algo_craw_wr"
TIDB_PASSWORD = "99FBD18120C777560A9451FB65A8E74F60CFBBD3"
TIDB_DATABASE = "da_crawler_dw"

# å†™å…¥â€œå·²ç­›é€‰ç”¨äºæ¨é€å‘Šè­¦â€çš„è®°å½•çš„æ–°è¡¨ï¼ˆå¤šäº† summary å’Œ event_levelï¼‰
NOTIFY_TABLE = "dwd_idc_life_ent_soc_public_sentiment_battery_work_notify_mix_rt"

# ============ å­—æ®µä¸å±•ç¤º ============
FIELD_MAP = {
    "summary":      "æ–‡ç« æ‘˜è¦",
    "work_id":      "ä¸»è´´ID",
    "work_url":     "ä¸»è´´é“¾æ¥",
    "work_title":   "ä¸»è´´æ ‡é¢˜",
    "work_content": "æ­£æ–‡å†…å®¹",
    "publish_time": "å‘å¸ƒæ—¶é—´",
    "crawled_time": "æŠ“å–æ—¶é—´",
    "account_name": "ä½œè€…åç§°",
    "source":       "æ¥æºå¹³å°",
    "like_cnt":     "ç‚¹èµæ•°",
    "reply_cnt":    "è¯„è®ºæ•°",
    "forward_cnt":  "è½¬å‘æ•°",
    "content_senti":"å†…å®¹æƒ…æ„Ÿ",
    "ocr_content":  "OCRè¯†åˆ«å†…å®¹"
}

ORDERED_FIELDS = [
    "source", "work_url", "publish_time", "account_name",
    "summary",
    "work_title", "work_content",
    "like_cnt", "reply_cnt", "forward_cnt"
]

ADVICE_BY_SEVERITY = {
    "ä½": "è¯·ç›¸å…³äººå‘˜äº†è§£",
    "ä¸­": "è¯·ç›¸å…³äººå‘˜å…³æ³¨",
    "é«˜": "è¯·ç›¸å…³äººå‘˜é‡ç‚¹å…³æ³¨",
}

# ============ å·¥å…·å‡½æ•° ============
def double_base64_decode(s: str) -> str:
    try:
        return base64.b64decode(base64.b64decode(s)).decode("utf-8")
    except Exception:
        return f"[è§£ç å¤±è´¥]{s}"

def truncate_text(text: str, limit=200) -> str:
    if text is None:
        return ""
    text = str(text)
    return text[:limit] + "..." if len(text) > limit else text

def map_senti(val):
    try:
        v = int(val)
    except Exception:
        return str(val)
    return {-1: "è´Ÿé¢", 0: "ä¸­æ€§", 1: "æ­£é¢"}.get(v, str(v))

def call_chat_completion_stream(prompt: str, model: str = "azure-gpt-4o") -> str:
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": True
    }
    result_chunks = []
    with requests.post(API_URL, headers=HEADERS, data=json.dumps(payload), stream=True, timeout=300) as resp:
        if resp.status_code != 200:
            raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
        for raw_line in resp.iter_lines(chunk_size=1024, decode_unicode=False):
            if not raw_line:
                continue
            line = raw_line.strip()
            if not line.startswith(b"data:"):
                continue
            data_bytes = line[len(b"data:"):].strip()
            if data_bytes == b"[DONE]":
                break
            try:
                obj = json.loads(data_bytes.decode("utf-8"))
            except Exception:
                result_chunks.append(data_bytes.decode("utf-8", errors="ignore"))
                continue
            choices = obj.get("choices") or []
            for ch in choices:
                delta = ch.get("delta") or {}
                chunk = delta.get("content")
                if chunk is None:
                    chunk = ch.get("content")
                if chunk is None:
                    msg = ch.get("message") or {}
                    chunk = msg.get("content")
                if chunk:
                    result_chunks.append(chunk)
    return "".join(result_chunks).strip()

def _extract_json_from_text(text: str) -> str:
    s = text.strip()
    # æ”¯æŒ ```json ... ``` ä»£ç å—
    fence = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", s, flags=re.IGNORECASE)
    if fence:
        return fence.group(1).strip()
    # ç®€å•æ‹¬å·åŒ¹é…å¯»æ‰¾é¦–ä¸ªå®Œæ•´ JSON å¯¹è±¡
    start = s.find("{")
    if start != -1:
        depth = 0
        for i in range(start, len(s)):
            c = s[i]
            if c == "{":
                depth += 1
            elif c == "}":
                depth -= 1
                if depth == 0:
                    return s[start:i+1].strip()
    return s

# ============ ç›¸å…³æ€§åˆ¤å®š ============
def build_related_gate_prompt(title: str, content: str, ocr: str) -> str:
    title = title or ""
    content = content or ""
    ocr = ocr or ""
    return (
        "è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸â€œç†æƒ³æ±½è½¦â€çš„ç”µæ± ç›¸å…³ã€‚"
        "æ ‡å‡†ï¼šå‡ºç°ç†æƒ³æ±½è½¦/ç†æƒ³ï¼ˆLi Auto/Liï¼‰çš„å“ç‰ŒæŒ‡å‘ï¼Œä¸”è¯é¢˜èšç„¦ç”µæ± ç›¸å…³è®®é¢˜ï¼ˆç”µæ± ã€ç»­èˆªã€å……ç”µã€å®‰å…¨ã€èµ·ç«ã€çˆ†ç‚¸ã€æ•…éšœã€ä½æ¸©ã€BMSç­‰ï¼‰ã€‚"
        "åªè¿”å›çº¯ JSONï¼Œä¸è¦ä»£ç å—æˆ–å…¶ä»–æ–‡å­—ï¼š"
        '{"related": "æ˜¯"} æˆ– {"related": "å¦"}'
        f"\næ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content}\nOCRï¼š{ocr}\n"
        "åªè¿”å›ä¸Šè¿° JSONã€‚"
    )

def parse_related_json(text: str) -> bool:
    raw = text.strip()
    json_str = _extract_json_from_text(raw)
    related = None
    try:
        obj = json.loads(json_str)
        if isinstance(obj, dict):
            val = str(obj.get("related", "")).strip()
            if val in ("æ˜¯", "å¦"):
                related = (val == "æ˜¯")
    except Exception:
        pass
    if related is None:
        # é™çº§å…³é”®è¯
        all_text = raw
        li_keywords = ["ç†æƒ³", "ç†æƒ³æ±½è½¦", "Li Auto", "LI Auto", "li auto", "Li", "ç†æƒ³L", "ç†æƒ³ONE", "ç†æƒ³L7", "ç†æƒ³L8", "ç†æƒ³L9", "ç†æƒ³i8", "ç†æƒ³i6"]
        batt_keywords = ["ç”µæ± ", "ç»­èˆª", "å……ç”µ", "èµ·ç«", "çˆ†ç‚¸", "æ¼æ¶²", "é¼“åŒ…", "å†…é˜»", "è¡°å‡", "ä½æ¸©", "BMS", "ç”µé‡", "SOC", "å®¹é‡" , "SOH"]
        has_li = any(k.lower() in all_text.lower() for k in li_keywords)
        has_batt = any(k.lower() in all_text.lower() for k in batt_keywords)
        related = bool(has_li and has_batt)
    return related

def check_related(data: dict) -> bool:
    title = data.get("work_title") or ""
    content = data.get("work_content") or ""
    ocr = data.get("ocr_content") or ""
    if isinstance(title, (dict, list)):
        title = json.dumps(title, ensure_ascii=False)
    if isinstance(content, (dict, list)):
        content = json.dumps(content, ensure_ascii=False)
    if isinstance(ocr, (dict, list)):
        ocr = json.dumps(ocr, ensure_ascii=False)
    try:
        llm_text = call_chat_completion_stream(build_related_gate_prompt(title, content, ocr), model="azure-gpt-4o")
        return parse_related_json(llm_text)
    except Exception:
        all_text = f"{title}\n{content}\n{ocr}"
        li_keywords = ["ç†æƒ³", "ç†æƒ³æ±½è½¦", "Li Auto", "LI Auto", "li auto", "Li", "ç†æƒ³L", "ç†æƒ³ONE", "ç†æƒ³L7", "ç†æƒ³L8", "ç†æƒ³L9", "ç†æƒ³i8", "ç†æƒ³i6"]
        batt_keywords = ["ç”µæ± ", "ç»­èˆª", "å……ç”µ", "èµ·ç«", "çˆ†ç‚¸", "æ¼æ¶²", "é¼“åŒ…", "å†…é˜»", "è¡°å‡", "ä½æ¸©", "BMS", "ç”µé‡", "SOC", "å®¹é‡" , "SOH"]
        has_li = any(k.lower() in all_text.lower() for k in li_keywords)
        has_batt = any(k.lower() in all_text.lower() for k in batt_keywords)
        return bool(has_li and has_batt)

# ============ æ‘˜è¦ä¸çƒˆåº¦ ============
def build_summary_prompt(title: str, content: str, ocr: str) -> str:
    title = title or ""
    content = content or ""
    ocr = ocr or ""
    return (
        "ä½ æ˜¯ä¼ä¸šèˆ†æƒ…åˆ†æåŠ©æ‰‹ã€‚è¯·é˜…è¯»ä¸»è´´æ ‡é¢˜ã€æ­£æ–‡ã€OCRè¯†åˆ«å†…å®¹ã€‚"
        "è¯·å®Œæˆï¼š"
        "1) ç”¨ä¸­æ–‡è¾“å‡ºçº¦50å­—çš„ä¸€æ®µäº‹ä»¶æ‘˜è¦ï¼ˆä¸åŒ…å«æ˜¯å¦ä¸ç†æƒ³æ±½è½¦ç”µæ± ç›¸å…³çš„åˆ¤æ–­ï¼‰ï¼›"
        "2) å•ç‹¬ç»™å‡ºäº‹ä»¶çƒˆåº¦ï¼Œä»…å¯ä¸ºï¼šä½/ä¸­/é«˜ï¼›"
        "ä¸¥æ ¼è¿”å›çº¯ JSON æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡å­—ï¼Œä¸è¦ä½¿ç”¨ä»£ç å—æˆ–åå¼•å·ï¼š"
        '{"summary": "<äº‹ä»¶æ‘˜è¦>", "severity": "<ä½|ä¸­|é«˜>"}'
        f"\næ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content}\nOCRï¼š{ocr}\n"
        "åªè¿”å›ä¸Šè¿° JSONã€‚"
    )

def parse_summary_json(text: str):
    raw = text.strip()
    json_str = _extract_json_from_text(raw)
    summary = None
    severity = None
    try:
        obj = json.loads(json_str)
        if isinstance(obj, dict):
            summary = str(obj.get("summary", "")).strip() or None
            severity = str(obj.get("severity", "")).strip() or None
    except Exception:
        pass
    if summary is None:
        summary = re.sub(r"^```(?:json)?\s*|\s*```$", "", raw, flags=re.IGNORECASE).strip()
    if severity not in ("ä½", "ä¸­", "é«˜"):
        m = re.search(r"(ä½|ä¸­|é«˜)", raw)
        severity = m.group(1) if m else "ä¸­"
    return summary, severity

def generate_summary_and_severity(data: dict):
    title = data.get("work_title") or ""
    content = data.get("work_content") or ""
    ocr = data.get("ocr_content") or ""
    if isinstance(title, (dict, list)):
        title = json.dumps(title, ensure_ascii=False)
    if isinstance(content, (dict, list)):
        content = json.dumps(content, ensure_ascii=False)
    if isinstance(ocr, (dict, list)):
        ocr = json.dumps(ocr, ensure_ascii=False)
    try:
        llm_text = call_chat_completion_stream(build_summary_prompt(title, content, ocr), model="azure-gpt-4o")
    except Exception as e:
        return f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥] {e}", "ä¸­"
    summary, severity = parse_summary_json(llm_text)
    return summary, severity

# ============ TiDB å†™å…¥ï¼šå‘Šè­¦è®°å½•è¡¨ ============
def get_tidb_connection():
    return pymysql.connect(
        host=TIDB_HOST,
        port=TIDB_PORT,
        user=TIDB_USER,
        password=TIDB_PASSWORD,
        database=TIDB_DATABASE,
        charset="utf8mb4",
        autocommit=True,
    )

def upsert_notify_record(data: dict, summary_text: str, severity: str):
    """
    å°†é€šè¿‡é—¨ç¦ï¼ˆéœ€è¦æ¨é€ï¼‰çš„è®°å½•å†™å…¥æ–°è¡¨ï¼š
    dwd_idc_life_ent_soc_public_sentiment_battery_work_notify_mix_rt
    é‡‡ç”¨ work_id åšå¹‚ç­‰ï¼ˆéœ€è¡¨ä¸Šæœ‰å”¯ä¸€é”®/ä¸»é”®çº¦æŸï¼‰ã€‚
    """
    cols = [
        "work_id", "work_url", "work_title", "work_content",
        "publish_time", "crawled_time", "account_name", "source",
        "like_cnt", "reply_cnt", "forward_cnt", "content_senti",
        "ocr_content", "summary", "event_level"
    ]
    values = [
        data.get("work_id"),
        data.get("work_url"),
        data.get("work_title"),
        data.get("work_content"),
        data.get("publish_time"),
        data.get("crawled_time"),
        data.get("account_name"),
        data.get("source"),
        data.get("like_cnt"),
        data.get("reply_cnt"),
        data.get("forward_cnt"),
        data.get("content_senti"),
        data.get("ocr_content"),
        summary_text,
        severity,
    ]
    placeholders = ",".join(["%s"] * len(cols))
    update_clause = ",".join([f"{c}=VALUES({c})" for c in cols if c != "work_id"])
    sql = f"""
    INSERT INTO {NOTIFY_TABLE} ({",".join(cols)})
    VALUES ({placeholders})
    ON DUPLICATE KEY UPDATE {update_clause}
    """
    try:
        conn = get_tidb_connection()
        with conn.cursor() as cur:
            cur.execute(sql, values)
        conn.close()
        print(f"âœ… å·²å†™å…¥/æ›´æ–°å‘Šè­¦è®°å½•è¡¨ {NOTIFY_TABLE}ï¼Œwork_id={data.get('work_id')}")
    except Exception as e:
        print(f"âŒ å†™å…¥å‘Šè­¦è®°å½•è¡¨å¤±è´¥: {e}")

# ============ æ¨é€ä¸»æµç¨‹ ============
def send_to_feishu(data: dict):
    # 1) é—¨ç¦ï¼šæ˜¯å¦ä¸ç†æƒ³æ±½è½¦ç”µæ± ç›¸å…³ï¼›ä¸ç›¸å…³åˆ™è·³è¿‡ï¼ˆä¸å†™å…¥æ–°è¡¨ï¼‰
    try:
        if not check_related(data):
            print("è·³è¿‡æ¨é€ï¼šæ¨¡å‹åˆ¤å®šä¸ç†æƒ³æ±½è½¦ç”µæ± ä¸ç›¸å…³ï¼ˆè®°å½•å·²æ–°å¢åˆ°æºè¡¨ï¼Œä½†ä¸å…¥å‘Šè­¦è¡¨ï¼‰ã€‚")
            return
    except Exception as e:
        print(f"ç›¸å…³æ€§åˆ¤å®šå¼‚å¸¸ï¼Œé»˜è®¤è·³è¿‡æ¨é€ï¼š{e}")
        return

    # 2) ç”Ÿæˆæ‘˜è¦ä¸çƒˆåº¦
    summary_text, severity = generate_summary_and_severity(data)

    # 3) å…ˆå†™å…¥â€œå‘Šè­¦è®°å½•è¡¨â€ï¼ˆå·²é€šè¿‡é—¨ç¦ç­›é€‰çš„é›†åˆï¼‰
    upsert_notify_record(data, summary_text, severity)

    # 4) ç»„ç»‡é£ä¹¦å†…å®¹å¹¶æ¨é€ï¼ˆé£ä¹¦å±•ç¤ºç”¨è§£ç /æˆªæ–­ï¼Œä¸å½±å“å…¥åº“åŸå€¼ï¼‰
    post_content = []
    advice = ADVICE_BY_SEVERITY.get(severity, ADVICE_BY_SEVERITY["ä¸­"])

    for k in ORDERED_FIELDS:
        if k == "summary":
            v = summary_text
        else:
            if k not in data:
                continue
            v = data.get(k)

            if isinstance(v, datetime.datetime):
                v = v.strftime("%Y-%m-%d %H:%M:%S")
            if v is None:
                v = ""

            if k in ("work_title", "work_content", "ocr_content"):
                v = truncate_text(v, limit=200)
            if k == "account_name":
                v = double_base64_decode(v)
            if k == "content_senti":
                v = map_senti(v)

        label = FIELD_MAP.get(k, k)
        post_content.append([
            {"tag": "text", "text": f"ã€{label}ã€‘: {v}"}
        ])

    # æœ«å°¾ï¼š@ æŒ‡å®šäºº + å¤„ç†æ„è§
    post_content.append([
        {"tag": "at", "user_id": FEISHU_OPEN_ID},
        {"tag": "text", "text": f" {advice}ï¼ˆçƒˆåº¦ï¼š{severity}ï¼‰"}
    ])

    payload = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": "ğŸ“¢ æ–°å¢è®°å½•å‘Šè­¦",
                    "content": post_content
                }
            }
        }
    }

    try:
        resp = requests.post(
            WEBHOOK_URL,
            headers={"Content-Type": "application/json; charset=utf-8"},
            data=json.dumps(payload, ensure_ascii=False).encode("utf-8")
        )
        ok = False
        if resp.status_code == 200:
            try:
                j = resp.json()
                if j.get("StatusCode") == 0 or j.get("code") == 0:
                    ok = True
            except Exception:
                ok = True
        if ok:
            print("âœ… é£ä¹¦æ¶ˆæ¯å·²å‘é€å¹¶@æŒ‡å®šäºº")
        else:
            print(f"âŒ é£ä¹¦æ¶ˆæ¯å‘é€å¤±è´¥: {resp.status_code} {resp.text}")
    except Exception as e:
        print(f"âŒ è°ƒç”¨é£ä¹¦æ¥å£å¼‚å¸¸: {e}")

# ============ è¿è¡Œå…¥å£ ============
if __name__ == "__main__":
    if len(sys.argv) >= 2:
        try:
            row_json_str = sys.argv[1]
            data = json.loads(row_json_str)
            send_to_feishu(data)
        except Exception as e:
            print(f"âŒ è§£æè¾“å…¥ JSON å¤±è´¥: {e}")
            sys.exit(1)
    else:
        # å¯é€‰ï¼šæœ¬åœ°æµ‹è¯•æ•°æ®
        test_data = {
            "id": 186,
            "work_id": "315bd20e7e7690e27f2859689ac4ba04",
            "work_url": "www.baidu.com",
            "work_title": "ç”µæ± çˆ†ç‚¸ï¼Œæ­»ä¼¤10ä½™äººï¼ˆç†æƒ³L9ç–‘ä¼¼äº‹æ•…ï¼‰",
            "work_content": "æ®ç°åœºæ¶ˆæ¯ï¼Œç–‘ä¼¼ç†æƒ³L9å‘ç”Ÿç”µæ± çˆ†ç‚¸å¯¼è‡´äººå‘˜ä¼¤äº¡ï¼Œå…·ä½“åŸå› è°ƒæŸ¥ä¸­",
            "publish_time": "2025-10-20 10:00:00",
            "crawled_time": "2025-10-20 10:05:00",
            "account_name": base64.b64encode(base64.b64encode("æµ‹è¯•è´¦å·".encode("utf-8"))).decode("utf-8"),
            "source": "å¾®åš",
            "like_cnt": 99,
            "reply_cnt": 12,
            "forward_cnt": 5,
            "content_senti": 0,
            "ocr_content": "æŠ¥é“ç§°ç†æƒ³æ±½è½¦æŸè½¦å‹ç”µæ± æ•…éšœå¼•å‘èµ·ç«çˆ†ç‚¸ï¼Œç°åœºæœ‰ä¼¤äº¡"
        }
        print("æœªæ£€æµ‹åˆ°è¾“å…¥å‚æ•°ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•æ¨é€...")
        send_to_feishu(test_data)