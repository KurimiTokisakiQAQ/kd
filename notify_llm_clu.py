# -*- coding: utf-8 -*-
# notify_llm.py
import requests
import json
import datetime
import base64
import sys
import re
import pymysql
import difflib

WEBHOOK_URL = "https://open.feishu.cn/open-apis/bot/v2/hook/c74b9141-1759-40e2-ae3a-50cc6389e1bc"
FEISHU_OPEN_ID = "ou_20b2bd16a8405b93019b7291ec5202c3"

# å¤§æ¨¡å‹è°ƒç”¨é…ç½®ï¼ˆå·²é‰´æƒï¼‰
API_URL = "https://llm-cmt-api.dev.fc.chj.cloud/agentops/chat/completions"
HEADERS = {
    "Content-Type": "application/json",
    # "Authorization": "Bearer <your-token>",
}

# TiDB è¿æ¥ä¿¡æ¯ï¼ˆä¸ç›‘æ§è¡¨åŒåº“ï¼‰
DB_CONFIG = {
    "host": "da-dw-tidb-10900.chj.cloud",
    "port": 3306,
    "user": "da_algo_craw_wr",
    "password": "99FBD18120C777560A9451FB65A8E74F60CFBBD3",
    "database": "da_crawler_dw",
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor
}

# é€šçŸ¥è½åº“çš„ç›®æ ‡è¡¨ï¼ˆæ–°å¢ similar_idã€summaryã€event_levelï¼‰
NOTIFY_TABLE = "dwd_idc_life_ent_soc_public_sentiment_battery_work_notify_mix_rt"

# å­—æ®µæ˜ å°„ä¸å±•ç¤ºé¡ºåº
FIELD_MAP = {
    "summary":      "æ–‡ç« æ‘˜è¦",
    "work_id":      "ä¸»è´´ID",
    "work_url":     "ä¸»è´´é“¾æ¥",
    "work_title":   "ä¸»è´´æ ‡é¢˜",
    "work_content": "æ­£æ–‡å†…å®¹",
    "publish_time": "å‘å¸ƒæ—¶é—´",
    "crawled_time": "æŠ“å–æ—¶é—´",
    "account_name": "ä½œè€…åç§°",
    "source":       "æ¥æºå¹³å°",
    "like_cnt":     "ç‚¹èµæ•°",
    "reply_cnt":    "è¯„è®ºæ•°",
    "forward_cnt":  "è½¬å‘æ•°",
    "content_senti":"å†…å®¹æƒ…æ„Ÿ",
    "ocr_content":  "OCRè¯†åˆ«å†…å®¹"
}
ORDERED_FIELDS = [
    "source", "work_url", "publish_time", "account_name",
    "summary",
]
ADVICE_BY_SEVERITY = {"ä½": "è¯·ç›¸å…³äººå‘˜äº†è§£", "ä¸­": "è¯·ç›¸å…³äººå‘˜å…³æ³¨", "é«˜": "è¯·ç›¸å…³äººå‘˜é‡ç‚¹å…³æ³¨"}

# ================= å…¬å…±å·¥å…· =================
def double_base64_decode(s: str) -> str:
    try:
        return base64.b64decode(base64.b64decode(s)).decode("utf-8")
    except Exception:
        return f"[è§£ç å¤±è´¥]{s}"

def truncate_text(text: str, limit=200) -> str:
    if text is None:
        return ""
    text = str(text)
    return text[:limit] + "..." if len(text) > limit else text

def map_senti(val):
    try:
        v = int(val)
    except Exception:
        return str(val)
    return {-1: "è´Ÿé¢", 0: "ä¸­æ€§", 1: "æ­£é¢"}.get(v, str(v))

def to_datetime(v):
    if isinstance(v, datetime.datetime):
        return v
    if isinstance(v, (int, float)):
        try:
            return datetime.datetime.fromtimestamp(v)
        except Exception:
            pass
    if isinstance(v, str):
        for fmt in ("%Y-%m-%d %H:%M:%S.%f", "%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S.%f", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%d"):
            try:
                return datetime.datetime.strptime(v.strip(), fmt)
            except Exception:
                continue
        try:
            return datetime.datetime.fromisoformat(v.strip())
        except Exception:
            pass
    return datetime.datetime.now()

def call_chat_completion_stream(prompt: str, model: str = "azure-gpt-4o") -> str:
    payload = {"model": model, "messages": [{"role": "user", "content": prompt}], "stream": True}
    result_chunks = []
    with requests.post(API_URL, headers=HEADERS, data=json.dumps(payload), stream=True, timeout=300) as resp:
        if resp.status_code != 200:
            raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
        for raw_line in resp.iter_lines(chunk_size=1024, decode_unicode=False):
            if not raw_line:
                continue
            line = raw_line.strip()
            if not line.startswith(b"data:"):
                continue
            data_bytes = line[len(b"data:"):].strip()
            if data_bytes == b"[DONE]":
                break
            try:
                obj = json.loads(data_bytes.decode("utf-8"))
            except Exception:
                result_chunks.append(data_bytes.decode("utf-8", errors="ignore"))
                continue
            choices = obj.get("choices") or []
            for ch in choices:
                delta = ch.get("delta") or {}
                chunk = delta.get("content") or ch.get("content") or (ch.get("message") or {}).get("content")
                if chunk:
                    result_chunks.append(chunk)
    return "".join(result_chunks).strip()

def _extract_json_from_text(text: str) -> str:
    s = text.strip()
    fence = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", s, flags=re.IGNORECASE)
    if fence:
        return fence.group(1).strip()
    start = s.find("{")
    if start != -1:
        depth = 0
        for i in range(start, len(s)):
            c = s[i]
            if c == "{":
                depth += 1
            elif c == "}":
                depth -= 1
                if depth == 0:
                    return s[start:i+1].strip()
    return s

# ================= ç»Ÿä¸€è¯„ä¼°ï¼ˆä¸¥æ ¼å›´ç»•é—®é¢˜çš„ä¸»ä½“èšç„¦ + å¯¹æ¯”è¯„æµ‹çº¦æŸï¼‰ =================
def build_evaluation_prompt(title: str, content: str, ocr: str) -> str:
    title = title or ""
    content = content or ""
    ocr = ocr or ""
    return (
        "è¯·ä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹å¸–å­ï¼Œå¹¶åªè¿”å›çº¯JSONï¼š"
        '{"focus":"æ˜¯|å¦","problem":"æ˜¯|å¦","summary":"çº¦50å­—ä¸­æ–‡æ‘˜è¦","severity":"ä½|ä¸­|é«˜"}ã€‚'
        "åˆ¤å®šè§„åˆ™ï¼š"
        "focus=æ˜¯ï¼šå¸–å­çš„ä¸»ä½“å¿…é¡»ä¸¥æ ¼å›´ç»•ç†æƒ³æ±½è½¦ï¼ˆLi Auto/ç†æƒ³ONE/L6/L7/L8/L9/i6/i8/Megaï¼‰çš„ç”µæ± æˆ–å¢ç¨‹å™¨çš„é—®é¢˜ã€‚"
        "problem=æ˜¯ï¼šæ˜ç¡®æŒ‡å‡ºç†æƒ³ç”µæ± æˆ–å¢ç¨‹å™¨å­˜åœ¨ä¸è¶³/ç¼ºé™·/é£é™©/æ•…éšœ/äº‹æ•…/æŠ•è¯‰/ç»´æƒ/å¬å›ç­‰é—®é¢˜ï¼›"
        "è‹¥ä¸ºå“ç‰Œå¯¹æ¯”/è¯„æµ‹/ä½“éªŒåˆ†äº«/ä¸€èˆ¬å»ºè®®/ç§‘æ™®ç­‰ï¼Œä¸”æœªæ˜ç¡®æŒ‡å‡ºç†æƒ³ç”µæ± æˆ–å¢ç¨‹å™¨æœ‰é—®é¢˜ï¼Œåˆ™problem=å¦ã€‚"
        f"\næ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content}\nOCRï¼š{ocr}\n"
        "åªè¿”å›ä¸Šè¿°JSONã€‚"
    )

def parse_evaluation_json(text: str):
    raw = text.strip()
    json_str = _extract_json_from_text(raw)
    focus = "å¦"
    problem = "å¦"
    summary = None
    severity = "ä¸­"
    try:
        obj = json.loads(json_str)
        if isinstance(obj, dict):
            fv = obj.get("focus"); pv = obj.get("problem")
            sv = obj.get("summary"); sev = obj.get("severity")
            if isinstance(fv, str) and fv.strip() in ("æ˜¯","å¦"):
                focus = fv.strip()
            if isinstance(pv, str) and pv.strip() in ("æ˜¯","å¦"):
                problem = pv.strip()
            if isinstance(sv, str):
                summary = sv.strip() or None
            if isinstance(sev, str) and sev.strip() in ("ä½","ä¸­","é«˜"):
                severity = sev.strip()
    except Exception:
        pass
    if summary is None:
        summary = re.sub(r"^```(?:json)?\s*|\s*```$", "", raw, flags=re.IGNORECASE).strip()
    return focus, problem, summary, severity

def evaluate_post(data: dict):
    title = data.get("work_title") or ""
    content = data.get("work_content") or ""
    ocr = data.get("ocr_content") or ""
    t = title if isinstance(title, str) else json.dumps(title, ensure_ascii=False)
    c = content if isinstance(content, str) else json.dumps(content, ensure_ascii=False)
    o = ocr if isinstance(ocr, str) else json.dumps(ocr, ensure_ascii=False)
    prompt = build_evaluation_prompt(t, c, o)
    try:
        llm_text = call_chat_completion_stream(prompt, model="azure-gpt-4o")
    except Exception as e:
        return "å¦", "å¦", f"[è¯„ä¼°å¤±è´¥] {e}", "ä¸­"
    return parse_evaluation_json(llm_text)

# ================= ç›¸ä¼¼èšç±»ä¸ç»Ÿè®¡ =================
def clean_text(s: str) -> str:
    if not s:
        return ""
    s = str(s).lower()
    s = re.sub(r"http[s]?://\S+", " ", s)           # å»æ‰URL
    s = re.sub(r"www\.\S+", " ", s)
    s = re.sub(r"@\S+", " ", s)                     # å»æ‰@æåŠ
    s = re.sub(r"#\S+#", " ", s)                    # å»æ‰è¯é¢˜
    s = re.sub(r"[^\w\u4e00-\u9fff]+", " ", s)      # éå­—æ¯æ•°å­—ä¸ä¸­æ–‡æ›¿æ¢ä¸ºç©ºæ ¼
    s = re.sub(r"\s+", " ", s).strip()
    return s

REPOST_HINTS = [
    "è½¬å‘", "è½¬è½½", "è½¬å¸–", "repost", "åˆ†äº«", "è½¬ä¸€ä¸‹", "å¿«æ¥çœ‹çœ‹", "via", "åŸæ–‡è§", "é“¾æ¥", "link"
]

def is_primary_informative(title: str, content: str) -> bool:
    """
    åˆ¤æ–­æ ‡é¢˜+æ­£æ–‡æ˜¯å¦ä¿¡æ¯é‡å……è¶³ï¼š
    - æ¸…æ´—åé•¿åº¦é˜ˆå€¼ï¼ˆ>=20å­—ç¬¦ï¼‰ï¼›
    - ä¸ä»¥â€œè½¬å‘/è½¬è½½/åˆ†äº«â€ç­‰ä¸ºä¸»ï¼›
    - éâ€œåªæœ‰é“¾æ¥/æåŠâ€çš„æƒ…å†µã€‚
    """
    t = clean_text(title)
    c = clean_text(content)
    combo = f"{t} {c}".strip()

    if len(combo) < 20:
        return False

    # åŒ…å«æ˜æ˜¾è½¬å‘æç¤ºä¸”æ­£æ–‡ä¿¡æ¯é‡è¾ƒå¼±æ—¶ï¼Œè§†ä¸ºä¸å……è¶³
    if any(h in (title or "").lower() or h in (content or "").lower() for h in REPOST_HINTS):
        # å¦‚æœå»æ‰é“¾æ¥/æåŠåé•¿åº¦ä»è¾ƒçŸ­åˆ™åˆ¤å®šä¸ºè½¬å‘ä¸ºä¸»
        if len(combo) < 40:
            return False

    # ä»…é“¾æ¥æˆ–ä»…æåŠçš„å¼±æ–‡æœ¬
    if not re.search(r"[\u4e00-\u9fff\w]{10,}", combo):
        return False

    return True

def choose_text_for_similarity(title: str, content: str, ocr: str) -> tuple:
    """
    è¿”å›ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„æ–‡æœ¬åŠæ¥æºç±»å‹ï¼š
    - ä¼˜å…ˆä½¿ç”¨æ ‡é¢˜+æ­£æ–‡ï¼ˆä¿¡æ¯é‡å……è¶³æ—¶ï¼‰ï¼›
    - å¦åˆ™ä½¿ç”¨ OCR æ–‡æœ¬ï¼›
    - è‹¥ä¸¤è€…éƒ½ä¸ºç©ºï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if is_primary_informative(title, content):
        return clean_text(title) + " " + clean_text(content), "primary"
    ocr_clean = clean_text(ocr)
    if len(ocr_clean) >= 10:
        return ocr_clean, "ocr"
    return "", "none"

def text_similarity(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    return difflib.SequenceMatcher(None, a, b).ratio()

def find_similar_id(conn, data: dict, lookback_days: int = 30, threshold: float = 0.72) -> str:
    """
    åœ¨é€šçŸ¥è¡¨ä¸­æŸ¥æ‰¾ç›¸ä¼¼ä¸»è´´çš„ç±»ç°‡ï¼š
    - å¯¹â€œæ–°è®°å½•â€å’Œâ€œå€™é€‰å†å²è®°å½•â€åˆ†åˆ«é€‰æ‹©ç”¨äºç›¸ä¼¼è®¡ç®—çš„æ–‡æœ¬æºï¼ˆprimary ä¼˜å…ˆï¼Œæ— æ³•åˆ¤æ–­åˆ™ç”¨ ocrï¼‰ï¼›
    - è¿‘ lookback_days æ—¶é—´çª—å†…ï¼Œè‹¥å­˜åœ¨ç›¸ä¼¼åº¦ >= threshold çš„è®°å½•ï¼Œæ²¿ç”¨å…¶ similar_idï¼›
    - å¦åˆ™è¿”å›æœ¬è®°å½•çš„ work_id ä½œä¸ºæ–°ç°‡ similar_idã€‚
    """
    new_text, new_src = choose_text_for_similarity(
        data.get("work_title"), data.get("work_content"), data.get("ocr_content")
    )
    pub_dt = to_datetime(data.get("publish_time"))
    start_dt = pub_dt - datetime.timedelta(days=lookback_days)

    best_sim = 0.0
    best_similar_id = None

    try:
        with conn.cursor() as cursor:
            sql = f"""
                SELECT work_id, similar_id, work_title, work_content, ocr_content
                FROM {NOTIFY_TABLE}
                WHERE publish_time >= %s
            """
            cursor.execute(sql, (start_dt,))
            rows = cursor.fetchall() or []
            for r in rows:
                cand_text, cand_src = choose_text_for_similarity(
                    r.get("work_title"), r.get("work_content"), r.get("ocr_content")
                )
                sim = text_similarity(new_text, cand_text)
                if sim > best_sim:
                    best_sim = sim
                    best_similar_id = r.get("similar_id") or r.get("work_id")
    except Exception as e:
        print(f"âš ï¸ æŸ¥æ‰¾ç›¸ä¼¼ç±»ç°‡å¼‚å¸¸ï¼Œå›é€€ä¸ºè‡ªèº« work_idï¼š{e}")

    if best_sim >= threshold and best_similar_id:
        return str(best_similar_id)
    return str(data.get("work_id") or "")

def compute_similar_counts(conn, similar_id: str, publish_time: datetime.datetime):
    """
    è®¡ç®—ï¼š
    - å½“æ—¥å†…ç›¸ä¼¼ä¸»è´´æ•°ï¼ˆåŒä¸€å¤©çš„è®°å½•æ•°ï¼‰
    - ä¸ƒæ—¥å†…ç›¸ä¼¼ä¸»è´´æ•°ï¼ˆä»¥è¯¥å‘å¸ƒæ—¥ä¸ºæˆªæ­¢ï¼Œå‘å‰å«å½“å¤©å…±7ä¸ªè‡ªç„¶æ—¥ï¼‰
    å‡åŒ…å«å½“å‰è¿™æ¡è®°å½•æœ¬èº«ã€‚
    """
    pub_dt = to_datetime(publish_time)
    day_str = pub_dt.strftime("%Y-%m-%d")
    start_7_date = (pub_dt.date() - datetime.timedelta(days=6)).strftime("%Y-%m-%d")
    end_7_date = pub_dt.strftime("%Y-%m-%d")

    day_cnt = 0
    seven_cnt = 0
    try:
        with conn.cursor() as cursor:
            # å•æ—¥
            sql_day = f"""
                SELECT COUNT(*) AS cnt FROM {NOTIFY_TABLE}
                WHERE similar_id = %s AND DATE(publish_time) = %s
            """
            cursor.execute(sql_day, (similar_id, day_str))
            day_cnt = (cursor.fetchone() or {}).get("cnt", 0) or 0

            # ä¸ƒæ—¥
            sql_7 = f"""
                SELECT COUNT(*) AS cnt FROM {NOTIFY_TABLE}
                WHERE similar_id = %s AND DATE(publish_time) BETWEEN %s AND %s
            """
            cursor.execute(sql_7, (similar_id, start_7_date, end_7_date))
            seven_cnt = (cursor.fetchone() or {}).get("cnt", 0) or 0
    except Exception as e:
        print(f"âš ï¸ ç»Ÿè®¡ç›¸ä¼¼ä¸»è´´æ•°é‡å¼‚å¸¸: {e}")
    return int(seven_cnt), int(day_cnt)

# ================= æ¨é€æ•°æ®è½åº“ =================
def _safe_int(v, default=None):
    if v is None:
        return default
    try:
        return int(v)
    except Exception:
        return default

def upsert_notify_and_counts(data: dict, summary_text: str, severity: str):
    """
    å…ˆç¡®å®š similar_id -> æ’å…¥/æ›´æ–°é€šçŸ¥è®°å½• -> ç»Ÿè®¡ä¸ƒæ—¥/å½“æ—¥ç›¸ä¼¼ä¸»è´´æ•°ã€‚
    è¿”å› (ok, similar_id, seven_cnt, day_cnt)ã€‚
    """
    conn = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        conn.autocommit(True)
        similar_id = find_similar_id(conn, data)

        with conn.cursor() as cursor:
            sql = f"""
            INSERT INTO {NOTIFY_TABLE} (
                work_id, work_url, work_title, work_content,
                publish_time, crawled_time, account_name, source,
                like_cnt, reply_cnt, forward_cnt, content_senti,
                ocr_content, summary, event_level, similar_id
            ) VALUES (
                %(work_id)s, %(work_url)s, %(work_title)s, %(work_content)s,
                %(publish_time)s, %(crawled_time)s, %(account_name)s, %(source)s,
                %(like_cnt)s, %(reply_cnt)s, %(forward_cnt)s, %(content_senti)s,
                %(ocr_content)s, %(summary)s, %(event_level)s, %(similar_id)s
            )
            ON DUPLICATE KEY UPDATE
                work_url=VALUES(work_url),
                work_title=VALUES(work_title),
                work_content=VALUES(work_content),
                publish_time=VALUES(publish_time),
                crawled_time=VALUES(crawled_time),
                account_name=VALUES(account_name),
                source=VALUES(source),
                like_cnt=VALUES(like_cnt),
                reply_cnt=VALUES(reply_cnt),
                forward_cnt=VALUES(forward_cnt),
                content_senti=VALUES(content_senti),
                ocr_content=VALUES(ocr_content),
                summary=VALUES(summary),
                event_level=VALUES(event_level),
                similar_id=VALUES(similar_id)
            """
            params = {
                "work_id": data.get("work_id"),
                "work_url": data.get("work_url"),
                "work_title": data.get("work_title"),
                "work_content": data.get("work_content"),
                "publish_time": to_datetime(data.get("publish_time")),
                "crawled_time": to_datetime(data.get("crawled_time")),
                "account_name": data.get("account_name"),
                "source": data.get("source"),
                "like_cnt": _safe_int(data.get("like_cnt"), default=-99),
                "reply_cnt": _safe_int(data.get("reply_cnt"), default=-99),
                "forward_cnt": _safe_int(data.get("forward_cnt"), default=-99),
                "content_senti": _safe_int(data.get("content_senti")),
                "ocr_content": data.get("ocr_content"),
                "summary": summary_text,
                "event_level": severity,
                "similar_id": similar_id
            }
            cursor.execute(sql, params)
            print("âœ… é€šçŸ¥æ•°æ®å·²è½åº“åˆ° TiDB é€šçŸ¥è¡¨ï¼ˆå« similar_idï¼‰")

        seven_cnt, day_cnt = compute_similar_counts(conn, similar_id, params["publish_time"])
        return True, similar_id, seven_cnt, day_cnt
    except Exception as e:
        print(f"âŒ é€šçŸ¥æ•°æ®è½åº“æˆ–ç»Ÿè®¡å¤±è´¥: {e}")
        return False, None, 0, 0
    finally:
        try:
            if conn:
                conn.close()
        except Exception:
            pass

# ================= æ¨é€æµç¨‹ï¼ˆè¯„ä¼° -> è½åº“ -> ç»Ÿè®¡ -> æ¨é€ï¼‰ =================
def send_to_feishu(data: dict):
    # 1) å¤§æ¨¡å‹è¯„ä¼°ï¼šä¸»ä½“ä¸¥æ ¼èšç„¦ + æ˜ç¡®æŒ‡å‡ºé—®é¢˜ï¼Œæ‰å…è®¸ç»§ç»­
    focus, problem, summary_text, severity = evaluate_post(data)
    if focus != "æ˜¯":
        print("è·³è¿‡ï¼šä¸»ä½“æœªä¸¥æ ¼èšç„¦ç†æƒ³æ±½è½¦çš„ç”µæ± æˆ–å¢ç¨‹å™¨é—®é¢˜ã€‚")
        return False
    if problem != "æ˜¯":
        print("è·³è¿‡ï¼šæœªæ˜ç¡®æŒ‡å‡ºç†æƒ³ç”µæ± æˆ–å¢ç¨‹å™¨å­˜åœ¨é—®é¢˜ï¼ˆå¯¹æ¯”/è¯„æµ‹æœªæ˜ç¡®æŒ‡å‡ºé—®é¢˜ä¸æ¨é€ï¼‰ã€‚")
        return False

    # 2) å…ˆè½è¡¨ + è®¡ç®—ç›¸ä¼¼æ•°é‡
    ok_db, similar_id, seven_cnt, day_cnt = upsert_notify_and_counts(data, summary_text, severity)
    if not ok_db:
        print("âŒ ç”±äºè½åº“/ç»Ÿè®¡å¤±è´¥ï¼Œæœ¬æ¬¡ä¸è¿›è¡Œé£ä¹¦æ¨é€ã€‚")
        return False

    # 3) æ„å»ºé£ä¹¦æ¶ˆæ¯ï¼ˆé™„å¸¦ä¸ƒæ—¥/å½“æ—¥ç›¸ä¼¼ä¸»è´´æ•°é‡ï¼‰
    advice = ADVICE_BY_SEVERITY.get(severity, ADVICE_BY_SEVERITY["ä¸­"])

    post_content = []
    for k in ORDERED_FIELDS:
        v = summary_text if k == "summary" else data.get(k)
        if isinstance(v, datetime.datetime):
            v = v.strftime("%Y-%m-%d %H:%M:%S")
        if v is None:
            v = ""
        if k in ("work_title", "work_content", "ocr_content"):
            v = truncate_text(v, limit=200)
        if k == "account_name":
            v = double_base64_decode(v)
        if k == "content_senti":
            v = map_senti(v)
        label = FIELD_MAP.get(k, k)
        post_content.append([{"tag": "text", "text": f"ã€{label}ã€‘: {v}"}])

    # æ–°å¢ï¼šç›¸ä¼¼ä¸»è´´æ•°é‡
    post_content.append([{"tag": "text", "text": f"ã€ä¸ƒæ—¥å†…ç›¸ä¼¼ä¸»è´´æ•°é‡ã€‘: {seven_cnt}"}])
    post_content.append([{"tag": "text", "text": f"ã€å•æ—¥å†…ç›¸ä¼¼ä¸»è´´æ•°é‡ã€‘: {day_cnt}"}])

    # æé†’ä¸çƒˆåº¦
    post_content.append([
        {"tag": "at", "user_id": FEISHU_OPEN_ID},
        {"tag": "text", "text": f" {advice}ï¼ˆçƒˆåº¦ï¼š{severity}ï¼‰"}
    ])

    payload = {
        "msg_type": "post",
        "content": {
            "post": {"zh_cn": {"title": "ğŸ“¢ è´Ÿé¢èˆ†æƒ…å‘Šè­¦ï¼ˆç†æƒ³ç”µæ± /å¢ç¨‹å™¨ï¼‰", "content": post_content}}
        }
    }

    ok = False
    try:
        resp = requests.post(
            WEBHOOK_URL,
            headers={"Content-Type": "application/json; charset=utf-8"},
            data=json.dumps(payload, ensure_ascii=False).encode("utf-8")
        )
        if resp.status_code == 200:
            try:
                j = resp.json()
                if j.get("StatusCode") == 0 or j.get("code") == 0:
                    ok = True
            except Exception:
                ok = True
        if ok:
            print("âœ… é£ä¹¦æ¶ˆæ¯å·²å‘é€å¹¶@æŒ‡å®šäºº")
        else:
            print(f"âŒ é£ä¹¦æ¶ˆæ¯å‘é€å¤±è´¥: {resp.status_code} {resp.text}")
    except Exception as e:
        print(f"âŒ è°ƒç”¨é£ä¹¦æ¥å£å¼‚å¸¸: {e}")

    return ok

# ================= è¿è¡Œå…¥å£ =================
if __name__ == "__main__":
    if len(sys.argv) >= 2:
        try:
            row_json_str = sys.argv[1]
            data = json.loads(row_json_str)
            send_to_feishu(data)
        except Exception as e:
            print(f"âŒ è§£æè¾“å…¥ JSON å¤±è´¥: {e}")
            sys.exit(1)
    else:
        # ç¤ºä¾‹æµ‹è¯•æ•°æ®ï¼ˆä»…ç‹¬ç«‹è¿è¡Œæ—¶ä½¿ç”¨ï¼‰
        test_data = {
            "id": 186,
            "work_id": "315bd20e7e7690e27f2859689ac4ba04",
            "work_url": "www.baidu.com",
            "work_title": "ç†æƒ³L9ç”µæ± ä½æ¸©å……ç”µå¤±è´¥å¹¶å¤šæ¬¡æŠ¥é”™ï¼Œç”¨æˆ·æŠ•è¯‰",
            "work_content": "è½¦ä¸»ç§°ç†æƒ³L9åœ¨å¯’æ½®ä¸‹æ— æ³•å……ç”µä¸”é¢‘ç¹BMSæŠ¥é”™ï¼Œç»­èˆªå¤§å¹…ä¸‹é™ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£ï¼Œå·²å‘å‚å®¶æŠ•è¯‰ã€‚",
            "publish_time": datetime.datetime.now(),
            "crawled_time": datetime.datetime.now(),
            "account_name": base64.b64encode(base64.b64encode("æµ‹è¯•è´¦å·".encode("utf-8"))).decode("utf-8"),
            "source": "å¾®åš",
            "like_cnt": 99,
            "reply_cnt": 12,
            "forward_cnt": 5,
            "content_senti": -1,
            "ocr_content": "ç†æƒ³æ±½è½¦L9ä½æ¸©æ— æ³•å……ç”µé¢‘ç¹æŠ¥é”™ï¼Œç–‘ä¼¼BMSæ•…éšœï¼Œç”¨æˆ·ç»´æƒ"
        }
        print("æœªæ£€æµ‹åˆ°è¾“å…¥å‚æ•°ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæµ‹è¯•æ¨é€å¹¶è½åº“...")
        send_to_feishu(test_data)